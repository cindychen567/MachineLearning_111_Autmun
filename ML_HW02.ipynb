{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from array import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_file(train , label , size , col):\n",
    "    train_file = open(train , \"rb\")\n",
    "    label_file = open(label , \"rb\")\n",
    "\n",
    "    train_file.read(16)\n",
    "    label_file.read(8)\n",
    "    images = np.zeros((size , col) )\n",
    "    labels = np.zeros((size) )\n",
    "    for i in range(size):\n",
    "        for j in range(col):\n",
    "            images[i,j] = int.from_bytes(train_file.read(1),byteorder='big')\n",
    "        labels[i] = int.from_bytes(label_file.read(1),byteorder='big')\n",
    "\n",
    "    return images , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , y_train = unpack_file(\"/Users/cindychen/Documents/ML_hw02/train-images.idx3-ubyte\",\\\n",
    "    \"/Users/cindychen/Documents/ML_hw02/train-labels.idx1-ubyte\" , 60000 , 28*28)\n",
    "x_test , y_test = unpack_file(\"/Users/cindychen/Documents/ML_hw02/t10k-images.idx3-ubyte\", \\\n",
    "    \"/Users/cindychen/Documents/ML_hw02/t10k-labels.idx1-ubyte\" , 10000 , 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "re=np.zeros((10,28*28,32))\n",
    "for i in range(len(x_train)):\n",
    "    c = int(y_train[i])\n",
    "    for d in range(28*28):\n",
    "        slice = int(x_train[i][d] // 8)\n",
    "        re[c][d][slice]+=1\n",
    "\n",
    "for c in range(10):\n",
    "    for d in range(28*28):\n",
    "        count=0\n",
    "        for p in range(32):\n",
    "            count+=re[c][d][p]\n",
    "        re[c][d][:]/=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(log_scale , pred , ans):\n",
    "    print('Postirior (in log scale):',)\n",
    "    print('0:{}\\n1: {}\\n2: {}\\n3: {}\\n4: {}\\n5: {}\\n6: {}\\n7: {}\\n8: {}\\n9: {}'.format(\\\n",
    "        log_scale[0],log_scale[1],log_scale[2],log_scale[3],log_scale[4],log_scale[5],log_scale[6],log_scale[7],log_scale[8],log_scale[9]))\n",
    "    print('Prediction: {} , Ans: {}\\n'.format(pred , int(ans)))\n",
    "\n",
    "def plot_image(mat , divide):\n",
    "    for k in range(10):\n",
    "        print('{}:'.format(k))\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                print('1' if mat[k][i*28+j] > divide else '0' , end = ' ')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def __init__(self , option , train_data , train_label, test_data , test_label):\n",
    "        self.option = option\n",
    "        self.train_data = train_data\n",
    "        self.train_label = train_label\n",
    "        self.test_data = test_data\n",
    "        self.test_label = test_label\n",
    "        self.class_num = [i for i in range(10)]\n",
    "        self.mu = []\n",
    "        self.std = []\n",
    "        self.prior = self.count_prior()\n",
    "        self.count = []\n",
    "\n",
    "    def count_prior(self):\n",
    "        prior = []\n",
    "        for val in range(10):\n",
    "            idx = [i for i in range(len(self.train_label)) if self.train_label[i] == val]\n",
    "            n = len(idx)\n",
    "            prior.append(len(idx)/len(self.train_label))\n",
    "        return prior\n",
    "        \n",
    "    def count_mean(self , idx):\n",
    "        return  np.mean(self.train_data[idx] , axis = 0)\n",
    "\n",
    "    def count_std(self, idx):\n",
    "        var = np.var(self.train_data[idx] , axis = 0)\n",
    "        for i in range(len(var)):\n",
    "            if var[i] == 0:\n",
    "                var[i] = 1000\n",
    "        return var\n",
    "\n",
    "    def train_continue(self):\n",
    "        for val in range(10):\n",
    "            idx = [i for i in range(len(self.train_label)) if self.train_label[i] == val]\n",
    "            self.mu.append(self.count_mean(idx))\n",
    "            self.std.append(self.count_std(idx))\n",
    "        \n",
    "        likelihood = []\n",
    "        error = 0\n",
    "        for n in range(len(self.test_label)):\n",
    "            sample = self.test_data[n]\n",
    "            log_scale = []\n",
    "            for val in range(10):\n",
    "                v_mean = self.mu[val]\n",
    "                v_std = self.std[val] \n",
    "                probs = 0\n",
    "                for j in range(28*28):\n",
    "                    p = (1 / np.sqrt(2 * np.pi * v_std[j])) * np.exp(-np.square(sample[j] - v_mean[j])/(2 * v_std[j]))\n",
    "                    probs += np.log(max(1e-30 , p)) * (-1)\n",
    "\n",
    "                # probs += np.log(self.prior[val])\n",
    "                log_scale.append(probs)\n",
    "\n",
    "            log_scale /= np.sum(log_scale)\n",
    "            likelihood.append(log_scale)\n",
    "            pred = np.argmin(log_scale)\n",
    "            if pred != self.test_label[n]:\n",
    "                error += 1\n",
    "            show(log_scale , pred , self.test_label[n])\n",
    "\n",
    "        print(error/len(self.test_label))\n",
    "        plot_image(self.mu , 128)\n",
    "\n",
    "\n",
    "    def train_discrete(self):\n",
    "        count_map = np.zeros((10,28*28,32))\n",
    "        for i in range(len(self.train_label)):\n",
    "            num = int(self.train_label[i])\n",
    "            for d in range(28*28):\n",
    "                slice = int(self.train_data[i,d])//8\n",
    "                re[num][d][slice] += 1\n",
    "        for c in range(10):\n",
    "            for d in range(28*28):\n",
    "                count=0\n",
    "                for p in range(32):\n",
    "                    count+=re[c][d][p]\n",
    "                re[c][d][:] /= count\n",
    "\n",
    "        likelihood = []\n",
    "        error = 0\n",
    "        for i in range(len(self.test_label)):\n",
    "            sample = self.test_data[i]\n",
    "            log_scale = []\n",
    "            for val in range(10):\n",
    "                probs = 0\n",
    "                for j in range(28*28):\n",
    "                    p = np.log(max(1e-6 , re[val][j][int(sample[j]//8)]))\n",
    "                    probs += p\n",
    "                log_scale.append(probs)\n",
    "            \n",
    "            log_scale /= np.sum(log_scale)\n",
    "            likelihood.append(log_scale)\n",
    "            pred = np.argmin(log_scale)\n",
    "            if pred != self.test_label[i]:\n",
    "                error += 1\n",
    "            show(log_scale , pred , self.test_label[i])\n",
    "        \n",
    "        mat = np.zeros((10,28*28))\n",
    "        for i in range(10):\n",
    "            for j in range(28*28):\n",
    "                mat[i][j] = np.argmax(re[i][j])\n",
    "            \n",
    "        plot_image(mat , 16)\n",
    "        print(error/len(self.test_label))\n",
    "            \n",
    "    \n",
    "    def train(self):\n",
    "        if self.option == 0:\n",
    "            self.train_continue()\n",
    "        else:\n",
    "            self.train_discrete()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = Naive_Bayes(0 , x_train , y_train, x_test , y_test)\n",
    "c_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = Naive_Bayes(1 , x_train , y_train, x_test , y_test)\n",
    "d_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oneline Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class online_learning:\n",
    "    def __init__(self , a , b , data):\n",
    "        self.pri_a = a\n",
    "        self.pri_b = b\n",
    "        self.data = data\n",
    "        self.post_a = self.pri_a\n",
    "        self.post_b = self.pri_b\n",
    "        self.likelihood = 0\n",
    "    \n",
    "    ## The theta MLE of Binomial distribtion is a/N, where a is the number of head result.\n",
    "    def compute_likelihood(self, a, b):\n",
    "        times = a+b\n",
    "        p = a / times\n",
    "        L = 1\n",
    "        for i in range(a):\n",
    "            L *= p\n",
    "        for j in range(b):\n",
    "            L *= (1-p)\n",
    "        return L*self.factorial(times, a)\n",
    "\n",
    "    ## Computing Factorial\n",
    "    def factorial(self, n , k):\n",
    "        out = 1\n",
    "        for mul in range(n,k,-1):\n",
    "            out *= mul\n",
    "        for div in range(1,n-k+1):\n",
    "            out /= div\n",
    "        return out           \n",
    "\n",
    "    ## Computing Posterior \n",
    "    def compute_posterior(self , n):\n",
    "        a = 0\n",
    "        b = 0\n",
    "        for identity in self.data[n]:\n",
    "            if identity == '1':\n",
    "                a += 1\n",
    "            else:\n",
    "                b += 1           \n",
    "        return a , b\n",
    "\n",
    "    def train(self):\n",
    "        count = len(self.data)\n",
    "        for i in range(count):\n",
    "            a , b = self.compute_posterior(i)\n",
    "            self.likelihood = self.compute_likelihood(a,b)\n",
    "            self.post_a += a\n",
    "            self.post_b += b\n",
    "            self.show(i)\n",
    "            self.pri_a , self.pri_b = self.post_a , self.post_b\n",
    "            \n",
    "\n",
    "    def show(self , n):\n",
    "        print('case {}: {}\\nLikelihood:{}\\nBeta prior:     a = {} b = {} \\\n",
    "        \\nBeta posterior: a = {} b = {}\\n'.format(n+1, self.data[n],\\\n",
    "        self.likelihood , self.pri_a , self.pri_b , self.post_a , self.post_b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(dir):\n",
    "    f = open( dir , \"r\")\n",
    "    text = []\n",
    "    for line in f.readlines():\n",
    "        line = line.split('\\n')[0]\n",
    "        text.append(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 1: 0101010101001011010101\n",
      "Likelihood:0.16818809509277344\n",
      "Beta prior:     a = 10 b = 1         \n",
      "Beta posterior: a = 21 b = 12\n",
      "\n",
      "case 2: 0110101\n",
      "Likelihood:0.29375515303997485\n",
      "Beta prior:     a = 21 b = 12         \n",
      "Beta posterior: a = 25 b = 15\n",
      "\n",
      "case 3: 010110101101\n",
      "Likelihood:0.22860542417943355\n",
      "Beta prior:     a = 25 b = 15         \n",
      "Beta posterior: a = 32 b = 20\n",
      "\n",
      "case 4: 0101101011101011010\n",
      "Likelihood:0.18286870706509092\n",
      "Beta prior:     a = 32 b = 20         \n",
      "Beta posterior: a = 43 b = 28\n",
      "\n",
      "case 5: 111101100011110\n",
      "Likelihood:0.21430705488578333\n",
      "Beta prior:     a = 43 b = 28         \n",
      "Beta posterior: a = 53 b = 33\n",
      "\n",
      "case 6: 101110111000110\n",
      "Likelihood:0.20659760529408003\n",
      "Beta prior:     a = 53 b = 33         \n",
      "Beta posterior: a = 62 b = 39\n",
      "\n",
      "case 7: 1010010111\n",
      "Likelihood:0.25082265600000003\n",
      "Beta prior:     a = 62 b = 39         \n",
      "Beta posterior: a = 68 b = 43\n",
      "\n",
      "case 8: 11101110110\n",
      "Likelihood:0.2619678932864457\n",
      "Beta prior:     a = 68 b = 43         \n",
      "Beta posterior: a = 76 b = 46\n",
      "\n",
      "case 9: 01000111101\n",
      "Likelihood:0.23609128871506807\n",
      "Beta prior:     a = 76 b = 46         \n",
      "Beta posterior: a = 82 b = 51\n",
      "\n",
      "case 10: 110100111\n",
      "Likelihood:0.2731290961743637\n",
      "Beta prior:     a = 82 b = 51         \n",
      "Beta posterior: a = 88 b = 54\n",
      "\n",
      "case 11: 01101010111\n",
      "Likelihood:0.24384881449471854\n",
      "Beta prior:     a = 88 b = 54         \n",
      "Beta posterior: a = 95 b = 58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = open_file('/Users/cindychen/Documents/ML_hw02/testfile.txt')\n",
    "\n",
    "    a = 10\n",
    "    b = 1\n",
    "    model = online_learning(a , b , data)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
